{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from NeuralNetwork import loss, activation, layer, network, optimiser\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling XOR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "print(f'Input:\\n{X}', end='\\n'*2)\n",
    "\n",
    "Y = np.array([[0], [1],\n",
    "              [1], [0]])\n",
    "print(f'Output:\\n{X}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = network.Sequential()\n",
    "model.add_layer(layer.Dense(n_neurons=3, name='Layer_1', optimiser=optimiser.SGDMN(), freeze_weights=False, weights=np.array([[0.1, 0.2, 0.3],\n",
    "                                                                                                     [0.6, 0.4, 0.7]]),\n",
    "                                bias=np.array([[0, 0, 0]])))\n",
    "\n",
    "model.add_layer(activation.Sigmoid(name='Activation_1'))\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=1, name='Layer_2', optimiser=optimiser.SGDMN(), freeze_weights=False, weights=np.array([[0.1],\n",
    "                                                                                                     [0.4],\n",
    "                                                                                                     [0.9]]),\n",
    "                                bias = np.array([[0]])))\n",
    "\n",
    "model.add_layer(activation.Sigmoid(name='Activation_2'))\n",
    "\n",
    "model.compile(loss=loss.MSE(), inputs=X, target=Y, batch=4)\n",
    "print(model)\n",
    "\n",
    "model.train(epochs=500)\n",
    "\n",
    "pred_val = model.predict(inputs=X)\n",
    "print(f'Input:\\n{X}', end='\\n'*2)\n",
    "print(f'Predicted Value:\\n{pred_val}', end='\\n'*2)\n",
    "print(f'Actual Output:\\n{Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    loss = K.square(y_pred - y_true)\n",
    "    loss = loss * (1/8)\n",
    "    loss = K.sum(loss, axis=0)\n",
    "    return loss\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]], dtype=np.float)\n",
    "\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float)\n",
    "\n",
    "model_keras = Sequential()\n",
    "model_keras.add(Dense(units=3, activation='sigmoid', input_shape=(2,), name='first'))\n",
    "model_keras.add(Dense(units=1, activation='sigmoid', name='second'))\n",
    "\n",
    "model_keras.layers[0].set_weights([np.array([[0.1, 0.2, 0.3],\n",
    "                                              [0.6, 0.4, 0.7]]),\n",
    "                                   np.array([[0, 0, 0]]).reshape(-1)])\n",
    "\n",
    "model_keras.layers[1].set_weights([np.array([[0.1],\n",
    "                                             [0.4],\n",
    "                                             [0.9]]),\n",
    "                                   np.array([[0]]).reshape(-1)])\n",
    "\n",
    "model_keras.compile(loss=custom_mse,\n",
    "                    optimizer=optimizers.SGD(learning_rate=1, momentum=0.9, nesterov=True)\n",
    "                    )\n",
    "\n",
    "model_keras.fit(X, Y.reshape(-1), epochs=500, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].weights)\n",
    "print(model.layers[0].bias)\n",
    "\n",
    "print()\n",
    "\n",
    "print(model_keras.layers[0].get_weights())\n",
    "\n",
    "print()\n",
    "\n",
    "print(model.layers[2].weights)\n",
    "print(model.layers[2].bias)\n",
    "\n",
    "print()\n",
    "\n",
    "print(model_keras.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Regression\n",
    "## Modelling Interval For Noisy Sine Wave\n",
    "Prediction Interval at 0.977 - 0.023 ~ 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_predictable(x):\n",
    "    return x+np.sin(np.pi*x/2)\n",
    "\n",
    "\n",
    "def f(x, std=0.2):\n",
    "    return f_predictable(x)+np.random.randn(len(x))*std\n",
    "\n",
    "\n",
    "def get_data(num, start=0, end=4):\n",
    "        x = np.sort(np.random.rand(num)*(end-start)+start)\n",
    "        y = f(x)\n",
    "        return x.reshape(-1, 1), y\n",
    "\n",
    "x_train, y_train = get_data(num=20000)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "x_test, y_test = get_data(num=1000)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_train.reshape(-1), y=y_train.reshape(-1),\n",
    "                    mode='markers',\n",
    "                    name='Original Data'))\n",
    "fig.update_layout(title='Noisy Sine Wave')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_upper = network.Sequential()\n",
    "\n",
    "model_upper.add_layer(layer.Dense(n_neurons=100, name='Layer_1', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.01))\n",
    "\n",
    "model_upper.add_layer(activation.ReLU(name='Activation_1'))\n",
    "\n",
    "model_upper.add_layer(layer.Dense(n_neurons=100, name='Layer_2', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.01))\n",
    "\n",
    "model_upper.add_layer(activation.ReLU(name='Activation_2'))\n",
    "\n",
    "model_upper.add_layer(layer.Dense(n_neurons=100, name='Layer_3', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.01))\n",
    "\n",
    "model_upper.add_layer(activation.ReLU(name='Activation_3'))\n",
    "\n",
    "model_upper.add_layer(layer.Dense(n_neurons=1, name='Layer_4', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.01))\n",
    "\n",
    "model_upper.compile(loss=loss.Quantile(quantile=0.977), inputs=x_train, target=y_train, batch=24)\n",
    "\n",
    "model_upper.train(epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lower = network.Sequential()\n",
    "\n",
    "model_lower.add_layer(layer.Dense(n_neurons=100, name='Layer_1', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.01))\n",
    "\n",
    "model_lower.add_layer(activation.ReLU(name='Activation_1'))\n",
    "\n",
    "model_lower.add_layer(layer.Dense(n_neurons=100, name='Layer_2', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.01))\n",
    "\n",
    "model_lower.add_layer(activation.ReLU(name='Activation_2'))\n",
    "\n",
    "model_lower.add_layer(layer.Dense(n_neurons=100, name='Layer_3', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.01))\n",
    "\n",
    "model_lower.add_layer(activation.ReLU(name='Activation_3'))\n",
    "\n",
    "model_lower.add_layer(layer.Dense(n_neurons=1, name='Layer_4', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.01))\n",
    "\n",
    "model_lower.compile(loss=loss.Quantile(quantile=0.023), inputs=x_train, target=y_train, batch=24)\n",
    "\n",
    "model_lower.train(epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_train.reshape(-1), y=y_train.reshape(-1),\n",
    "                    mode='markers',\n",
    "                    name='Original Data'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_test.reshape(-1), y=model_upper.predict(x_test).reshape(-1),\n",
    "                    mode='lines',\n",
    "                    name='Upper Bound'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_test.reshape(-1), y=model_lower.predict(x_test).reshape(-1),\n",
    "                    mode='lines',\n",
    "                    name='Lower Bound'))\n",
    "\n",
    "fig.update_layout(title='Noisy Sine Wave With 95% Prediction Bounds')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "Modelling Multi Class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cat_images = np.random.randn(700, 2) + np.array([0, -3])\n",
    "mouse_images = np.random.randn(700, 2) + np.array([3, 3])\n",
    "dog_images = np.random.randn(700, 2) + np.array([-3, 3])\n",
    "\n",
    "feature_set = np.vstack([cat_images, mouse_images, dog_images])\n",
    "labels = np.array([0]*700 + [1]*700 + [2]*700)\n",
    "\n",
    "one_hot_labels = np.zeros((2100, 3))\n",
    "\n",
    "for i in range(2100):\n",
    "    one_hot_labels[i, labels[i]] = 1\n",
    "    \n",
    "dataset = pd.DataFrame(np.hstack([feature_set, labels.reshape(-1, 1)]), columns=['X','Y','Labels'])\n",
    "dataset.Labels = dataset.Labels.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(dataset, x=\"X\", y=\"Y\", color=\"Labels\", hover_data=[dataset.index])\n",
    "fig.update_layout(width=1000, height=800, title='Various Images Data Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = network.Sequential()\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=4, name='Layer_1', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.001))\n",
    "\n",
    "model.add_layer(activation.Sigmoid(name='Activation_1'))\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=3, name='Layer_2', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.001))\n",
    "\n",
    "model.add_layer(activation.Sigmoid(name='Activation_2'))\n",
    "\n",
    "model.compile(loss=loss.CrossEntropy(), inputs=feature_set, target=one_hot_labels, batch=16)\n",
    "\n",
    "model.train(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = np.argmax(model.predict(feature_set), axis=1)\n",
    "np.where((pred_val == labels)==False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SquaredHinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = -1*np.ones((2100, 3))\n",
    "\n",
    "for i in range(2100):\n",
    "    one_hot_labels[i, labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = network.Sequential()\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=4, name='Layer_1', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.001))\n",
    "\n",
    "model.add_layer(activation.Sigmoid(name='Activation_1'))\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=3, name='Layer_2', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.001))\n",
    "\n",
    "model.add_layer(activation.Tanh(name='Activation_2'))\n",
    "\n",
    "model.compile(loss=loss.SquaredHinge(), inputs=feature_set, target=one_hot_labels, batch=16)\n",
    "\n",
    "model.train(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = np.argmax(model.predict(feature_set), axis=1)\n",
    "np.where((pred_val == labels)==False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "Modelling Multi Label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cat_images = np.random.randn(700, 2) + np.array([0, -3])\n",
    "mouse_images = np.random.randn(700, 2) + np.array([3, 3])\n",
    "dog_images = np.random.randn(700, 2) + np.array([-3, 3])\n",
    "\n",
    "feature_set = np.vstack([cat_images, mouse_images, dog_images])\n",
    "labels = np.array([0]*700 + [1]*700 + [1]*700)\n",
    "\n",
    "one_hot_labels = np.zeros((2100, 3))\n",
    "\n",
    "for i in range(700):\n",
    "    one_hot_labels[i, labels[i]] = 1\n",
    "    \n",
    "for i in range(700, 2100):\n",
    "    one_hot_labels[i,1:] = 1\n",
    "    \n",
    "dataset = pd.DataFrame(np.hstack([feature_set, labels.reshape(-1, 1)]), columns=['X','Y','Labels'])\n",
    "dataset.Labels = dataset.Labels.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(dataset, x=\"X\", y=\"Y\", color=\"Labels\", hover_data=[dataset.index])\n",
    "fig.update_layout(width=1000, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network.Sequential()\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=4, name='Layer_1', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.001))\n",
    "\n",
    "model.add_layer(activation.Sigmoid(name='Activation_1'))\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=3, name='Layer_2', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.001))\n",
    "\n",
    "model.add_layer(activation.Sigmoid(name='Activation_2'))\n",
    "\n",
    "model.compile(loss=loss.CrossEntropy(), inputs=feature_set, target=one_hot_labels, batch=16)\n",
    "\n",
    "model.train(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_val = np.where(model.predict(feature_set)>0.5, 1, 0)\n",
    "np.where((pred_val == one_hot_labels)==False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SquaredHinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = -np.ones((2100, 3))\n",
    "\n",
    "for i in range(700):\n",
    "    one_hot_labels[i, labels[i]] = 1\n",
    "    \n",
    "for i in range(700, 2100):\n",
    "    one_hot_labels[i,1:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network.Sequential()\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=4, name='Layer_1', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.001))\n",
    "\n",
    "model.add_layer(activation.Sigmoid(name='Activation_1'))\n",
    "\n",
    "model.add_layer(layer.Dense(n_neurons=3, name='Layer_2', optimiser=optimiser.SGD(), freeze_weights=False, lr=0.001))\n",
    "\n",
    "model.add_layer(activation.Tanh(name='Activation_2'))\n",
    "\n",
    "model.compile(loss=loss.SquaredHinge(), inputs=feature_set, target=one_hot_labels, batch=16)\n",
    "\n",
    "model.train(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = np.where(model.predict(feature_set)>0, 1, -1)\n",
    "np.where((pred_val == one_hot_labels)==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
